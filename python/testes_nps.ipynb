{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.contingency import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções e dicionários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dict = {\n",
    "    .90: 1.645,\n",
    "    .95: 1.96\n",
    "}\n",
    "\n",
    "# z_dict = {99:   2.575,\n",
    "#           95:   1.960,\n",
    "#           90:   1.645,\n",
    "#           70:   1.035,\n",
    "#           67.5: 0.985}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificacao_nps(x):\n",
    "    if x <= 6:\n",
    "        return 'detrator'\n",
    "    if x <= 8:\n",
    "        return 'neutro'\n",
    "    if x > 8:\n",
    "        return 'promotor'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabela_esperado(df_, colunas, filtros = None):\n",
    "    \n",
    "    if filtros is not None:\n",
    "        \n",
    "        df = df_.copy()\n",
    "    \n",
    "        for k, v in filtros.items():\n",
    "            df = df[df[k].isin(v)]\n",
    "            \n",
    "    else:\n",
    "        df = df_.copy()\n",
    "\n",
    "    chi_sqr = df[colunas + ['NPS_GERAL']].copy()\n",
    "    chi_sqr['class_nps'] = chi_sqr['NPS_GERAL'].apply(classificacao_nps)\n",
    "    chi_sqr = chi_sqr[colunas + ['class_nps']].value_counts().reset_index(name = 'qtd')\n",
    "\n",
    "    return chi_sqr\n",
    "\n",
    "def relatorio_qui_quadrado(col1, col2, filtros = None, print_relatorio = True):\n",
    "\n",
    "    base_esperado = tabela_esperado(colunas = [col1, col2], filtros = filtros)\n",
    "    \n",
    "    tabela_cat_1 = pd.DataFrame()\n",
    "\n",
    "    for cat1 in base_esperado[col1].unique():\n",
    "\n",
    "        if print_relatorio:\n",
    "            print(cat1.upper())\n",
    "            print('-'* 100)\n",
    "\n",
    "        tabela_contingencia = []\n",
    "\n",
    "        for cat2 in base_esperado[col2].unique():\n",
    "            lista = list(base_esperado[(base_esperado[col1] == cat1) & (base_esperado[col2] == cat2)]['qtd'])\n",
    "\n",
    "            tabela_contingencia.append(lista)\n",
    "\n",
    "        tabela_contingencia = np.array(tabela_contingencia)\n",
    "\n",
    "        # Aplicando o teste qui-quadrado\n",
    "        alpha = 0.05\n",
    "        chi2_stat, p, dof, expected = chi2_contingency(tabela_contingencia)\n",
    "        valor_critico = chi2.ppf(1 - alpha, dof)\n",
    "        \n",
    "        if print_relatorio:\n",
    "\n",
    "            # Exibindo os resultados\n",
    "            print(f\"Qui-quadrado: {chi2_stat:.2f}\")\n",
    "            print(f\"Valor crítico: {valor_critico:.3f}\")\n",
    "            print(f\"P-valor: {p:.4f}\")\n",
    "            print(f\"Graus de liberdade: {dof}\")\n",
    "            print()\n",
    "            print(\"Frequências observadas:\")\n",
    "            print(tabela_contingencia)\n",
    "            print()\n",
    "            print(\"Frequências esperadas:\")\n",
    "            print(expected.round())\n",
    "            print()\n",
    "            print(\"Observado - esperado:\")\n",
    "            print(tabela_contingencia - expected.round())\n",
    "            print()\n",
    "\n",
    "            # Interpretação\n",
    "            print('Resultado:')\n",
    "            if p < alpha:\n",
    "                print(\"Rejeitamos a hipótese nula: há uma associação significativa entre os grupos e as categorias.\")\n",
    "            else:\n",
    "                print(\"Não rejeitamos a hipótese nula: não há evidências de associação significativa entre os grupos e as categorias.\")\n",
    "\n",
    "            print()\n",
    "        \n",
    "        tmp = {col1:[cat1],\n",
    "               'qui_quadrado':[f\"{chi2_stat:.2f}\"],\n",
    "               'valor_critico':[f\"{valor_critico:.3f}\"],\n",
    "               'p_valor':[f\"{p:.4f}\"]}\n",
    "        \n",
    "        df_tmp = pd.DataFrame(data = tmp)\n",
    "        \n",
    "        tabela_cat_1 = pd.concat([tabela_cat_1, df_tmp])\n",
    "        \n",
    "    return tabela_cat_1\n",
    "\n",
    "def margem_erro(p, n, d, z):\n",
    "\n",
    "    total = p + n + d\n",
    "\n",
    "    nps = (p - d) / total \n",
    "\n",
    "    var_nps = ((1 - nps)**2 * (p/total)) + ((0 - nps)**2 * (n/total)) + ((1 - nps)**2 * (d/total))\n",
    "\n",
    "    margem_erro = round(z * (var_nps/total)**(1/2) * 100, 1)\n",
    "    \n",
    "    return margem_erro\n",
    "\n",
    "def margem_erro_grupo(df, z):\n",
    "    \n",
    "    classificacoes = ['promotor', 'neutro', 'detrator']\n",
    "    valores = {classe: 0 for classe in classificacoes}\n",
    "    \n",
    "    for classe in classificacoes:\n",
    "        if classe in df.class_nps.unique():\n",
    "            valores[classe] = df[df['class_nps'] == classe].reset_index()['qtd'][0]\n",
    "    \n",
    "    margem = margem_erro(p = valores['promotor'], n = valores['neutro'], d = valores['detrator'], z = z)\n",
    "    \n",
    "    return margem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerar tabela de NPS usando bootstrap com repetição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap com repetição\n",
    "df_margem_erro = pd.DataFrame()\n",
    "\n",
    "for plataforma in ['Controle', 'Pós-pago']:\n",
    "    for cat in ['E-mail', 'WhatsApp', 'Correios', 'Aplicativo']:\n",
    "        for multa in ['Teve multa', 'Não teve multa']:\n",
    "            \n",
    "            print(f\"{plataforma} | {cat} | {multa}\")\n",
    "            \n",
    "            df_tmp = base_analitica_nps[(base_analitica_nps['COD_PLAT'] == plataforma) & \n",
    "                                        (base_analitica_nps['FLG_MULTA_ULTIMOS_12_MESES'] == multa) &\n",
    "                                        (base_analitica_nps['DSC_CATEGORIA_IMPRESSAO'] == cat)]\\\n",
    "                .copy().reset_index(drop = True)\n",
    "\n",
    "\n",
    "            for i in tqdm(range(10000)):\n",
    "\n",
    "                amostra_i = np.random.randint(low = 0, high = df_tmp.shape[0], size = df_tmp.shape[0]).tolist()\n",
    "                df_i = df_tmp.loc[amostra_i].copy()\n",
    "                df_i['class_nps'] = df_i['NPS_GERAL'].apply(classificacao_nps)\n",
    "\n",
    "                df_nps = df_i['class_nps'].value_counts()\n",
    "\n",
    "                total = df_nps.sum()\n",
    "                p = df_nps['promotor'] * 100 / total\n",
    "                d = df_nps['detrator'] * 100 / total\n",
    "\n",
    "                nps = round(p - d)\n",
    "\n",
    "                tmp = pd.DataFrame(data = {'COD_PLAT':[plataforma],\n",
    "                                           'DSC_CATEGORIA_IMPRESSAO':[cat],\n",
    "                                           'FLG_MULTA_ULTIMOS_12_MESES':[multa],\n",
    "                                           'nps': [nps]})\n",
    "\n",
    "                df_margem_erro = pd.concat([df_margem_erro, tmp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
